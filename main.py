import json
import os
import logging
from datetime import datetime
from typing import List, Dict
from prompt_optimizer import PromptOptimizer
from api_clients import DeepSeekAPI, QwenAPI

def setup_logging():
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    logger = logging.getLogger("Main")
    logger.setLevel(logging.INFO)
    
    # æ–‡ä»¶å¤„ç†å™¨
    fh = logging.FileHandler("process.log", encoding='utf-8')
    fh.setLevel(logging.DEBUG)
    fh_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    fh.setFormatter(fh_formatter)
    
    # æ§åˆ¶å°å¤„ç†å™¨
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch_formatter = logging.Formatter('%(message)s')
    ch.setFormatter(ch_formatter)
    
    logger.addHandler(fh)
    logger.addHandler(ch)
    return logger

def load_example_data(file_path: str) -> Dict:
    """åŠ è½½JSONæ•°æ®"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = f.read()
            try:
                return json.loads(data)
            except json.JSONDecodeError as je:
                logger = logging.getLogger()
                logger.error(f"JSONè§£æé”™è¯¯ {file_path}: {str(je)}")
                logger.error(f"åŸå§‹æ•°æ®å†…å®¹: {data}")
                raise
    except FileNotFoundError as fe:
        logger = logging.getLogger()
        logger.error(f"æ‰¾ä¸åˆ°æ–‡ä»¶ {file_path}")
        raise
    except Exception as e:
        logger = logging.getLogger()
        logger.error(f"åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
        logger.error("è¯¦ç»†é”™è¯¯ä¿¡æ¯:", exc_info=True)
        raise

def process_input_data(input_data):
    """å¤„ç†è¾“å…¥æ•°æ®ï¼Œç¡®ä¿è¿”å›ç–¾ç—…åˆ—è¡¨"""
    logger = logging.getLogger()
    try:
        if input_data is None:
            logger.error("è¾“å…¥æ•°æ®ä¸ºNone")
            return []
            
        if isinstance(input_data, list):
            result = [str(d).strip() for d in input_data if d is not None]
            logger.debug(f"å¤„ç†åˆ—è¡¨æ•°æ®: {input_data} -> {result}")
            return result
            
        elif isinstance(input_data, dict):
            if 'diseases' in input_data:
                diseases = input_data['diseases']
                if isinstance(diseases, list):
                    result = [str(d).strip() for d in diseases if d is not None]
                    logger.debug(f"å¤„ç†å­—å…¸ä¸­çš„diseasesåˆ—è¡¨: {diseases} -> {result}")
                    return result
                elif diseases is not None:
                    result = [str(diseases).strip()]
                    logger.debug(f"å¤„ç†å­—å…¸ä¸­çš„å•ä¸ªdisease: {diseases} -> {result}")
                    return result
            
            logger.warning(f"å­—å…¸ä¸­æ²¡æœ‰æ‰¾åˆ°diseasesé”®: {input_data}")
            return []
            
        else:
            logger.warning(f"æœªçŸ¥çš„è¾“å…¥æ•°æ®ç±»å‹: {type(input_data)}")
            return []
            
    except Exception as e:
        logger.error(f"å¤„ç†è¾“å…¥æ•°æ®æ—¶å‡ºé”™: {str(e)}", exc_info=True)
        logger.error(f"é—®é¢˜æ•°æ®: {input_data}")
        return []

def calculate_overlap(predicted_diseases: List[str], ground_truth: List[str]) -> float:
    """è®¡ç®—é¢„æµ‹ç–¾ç—…åˆ—è¡¨å’ŒçœŸå®ç–¾ç—…åˆ—è¡¨çš„é‡åˆåº¦"""
    if not predicted_diseases or not ground_truth:
        return 0.0
    
    # å°†ç–¾ç—…åç§°æ ‡å‡†åŒ–
    pred_set = {d.strip('.,ï¼Œã€‚ \t').lower() for d in predicted_diseases if d}
    truth_set = {d.strip('.,ï¼Œã€‚ \t').lower() for d in ground_truth if d}
    
    if not pred_set or not truth_set:
        return 0.0
        
    overlap = pred_set & truth_set
    union = pred_set | truth_set
    
    # è®°å½•é‡åˆå’Œå·®å¼‚
    overlap_diseases = list(overlap)
    missed_diseases = list(truth_set - pred_set)
    extra_diseases = list(pred_set - truth_set)
    
    return {
        'score': len(overlap) / len(union),
        'overlap_diseases': overlap_diseases,
        'missed_diseases': missed_diseases,
        'extra_diseases': extra_diseases
    }

def main():
    logger = setup_logging()
    logger.info("\n" + "="*60)
    logger.info("åŒ»ç–—å½±åƒè¯Šæ–­promptä¼˜åŒ–ç³»ç»Ÿ")
    logger.info("="*60 + "\n")
    
    logger.info("ã€åˆå§‹åŒ–é˜¶æ®µã€‘")
    logger.info("-"*40)
    
    # åˆå§‹åŒ–APIå®¢æˆ·ç«¯å’Œä¼˜åŒ–å™¨
    logger.info("âš¡ æ­£åœ¨åˆå§‹åŒ–APIå®¢æˆ·ç«¯...")
    try:
        deepseek_optimizer = DeepSeekAPI(role="optimizer")
        logger.info("âœ“ DeepSeekä¼˜åŒ–å™¨APIå°±ç»ª (ç”¨é€”ï¼šç”Ÿæˆä¼˜åŒ–åçš„prompt)")
        
        deepseek_analyzer = DeepSeekAPI(role="analyzer")
        logger.info("âœ“ DeepSeekåˆ†æå™¨APIå°±ç»ª (ç”¨é€”ï¼šåˆ†æä¼˜åŒ–ç»“æœ)")
        
        qwen_client = QwenAPI()
        logger.info("âœ“ é€šä¹‰åƒé—®APIå°±ç»ª (ç”¨é€”ï¼šæ‰§è¡Œç–¾ç—…é¢„æµ‹)")
        
        optimizer = PromptOptimizer(deepseek_optimizer, deepseek_analyzer, qwen_client)
        logger.info("âœ“ Promptä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        logger.error(f"âœ— APIåˆå§‹åŒ–å¤±è´¥: {str(e)}")
        raise
    
    logger.info("\nã€æ•°æ®åŠ è½½é˜¶æ®µã€‘")
    logger.info("-"*40)
    
    # è·å–æ‰€æœ‰è¾“å…¥æ–‡ä»¶
    input_dir = os.path.join('data', 'inputs')
    input_files = [f for f in os.listdir(input_dir) if f.endswith('_exam_input.json')]
    
    # åˆ›å»ºresultsç›®å½•
    os.makedirs('results', exist_ok=True)
    
    # ç¡®ä¿logsç›®å½•å­˜åœ¨
    logs_dir = 'logs'
    os.makedirs(logs_dir, exist_ok=True)
    
    # åˆ›å»ºæœ¬æ¬¡è¿è¡Œçš„æ—¥å¿—æ–‡ä»¶
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = os.path.join(logs_dir, f'optimization_{timestamp}.log')
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
    logger.addHandler(file_handler)
    
    max_iterations = 10
    best_prompt = optimizer.base_prompt_template
    best_avg_overlap = 0.0
    stagnation_count = 0  # è¿ç»­åœæ»æ¬¡æ•°
    last_best_accuracy = 0.0  # ä¸Šä¸€æ¬¡æœ€ä½³å‡†ç¡®ç‡
    
    # é¢„å…ˆåŠ è½½æ‰€æœ‰æ•°æ®
    logger.info(f"æ­£åœ¨ä» {input_dir} åŠ è½½æµ‹è¯•æ¡ˆä¾‹...")
    all_cases = []
    for input_file in input_files:
        case_id = input_file.replace('_exam_input.json', '')
        try:
            input_path = os.path.join(input_dir, input_file)
            gt_path = os.path.join('data', 'gts', f'{case_id}_gt.json')
            
            case_data = {
                'case_id': case_id,
                'input_data': load_example_data(input_path),
                'gt_data': load_example_data(gt_path)
            }
            all_cases.append(case_data)
            logger.info(f"âœ“ æ¡ˆä¾‹ {case_id:<15} æ•°æ®å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡")
        except Exception as e:
            logger.error(f"âœ— æ¡ˆä¾‹ {case_id:<15} æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            continue
    
    logger.info(f"\n>>> æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(all_cases)} ä¸ªæœ‰æ•ˆæ¡ˆä¾‹")
    
    logger.info("\nã€ä¼˜åŒ–è¿­ä»£é˜¶æ®µã€‘")
    logger.info("-"*40)
    
    for i in range(max_iterations):
        logger.info("\n" + "="*60)
        logger.info(f"è¿­ä»£è½®æ¬¡: {i+1}/{max_iterations}")
        logger.info(f"å½“å‰æœ€ä½³é‡åˆåº¦: {best_avg_overlap:.2%}")
        logger.info("="*60 + "\n")
        
        logger.info("1ï¸âƒ£ æ­£åœ¨ç”Ÿæˆæ–°çš„prompt...")
        try:
            optimization_context = {
                'base_prompt': best_prompt,
                'all_cases': all_cases[:3],  # å…ˆç”¨å‰ä¸‰ä¸ªæ¡ˆä¾‹ä¼˜åŒ–
                'iteration': i + 1,
                'previous_results': {
                    'best_overlap': best_avg_overlap,
                    'iteration': i
                }
            }
            
            new_prompt = optimizer.generate_prompt(optimization_context)
            logger.info("âœ“ æ–°promptç”ŸæˆæˆåŠŸ")
            
            # æ£€æŸ¥ä¼˜åŒ–è¿›åº¦
            progress = optimizer.evaluate_optimization_progress()
            logger.info(f"\nä¼˜åŒ–çŠ¶æ€: {progress['message']}")
            logger.info(f"æœ€ä½³å‡†ç¡®ç‡: {progress['best_accuracy']:.2%}")
            
            # æ ¹æ®ä¼˜åŒ–è¿›åº¦è°ƒæ•´ç­–ç•¥
            if progress['status'] == 'stagnating':
                stagnation_count += 1
                if stagnation_count >= 3:  # è¿ç»­3è½®æ— æ˜¾è‘—æ”¹è¿›
                    logger.info("\nğŸ”„ æ£€æµ‹åˆ°ä¼˜åŒ–åœæ»ï¼Œå°è¯•è°ƒæ•´ç­–ç•¥...")
                    # å¢åŠ æµ‹è¯•æ¡ˆä¾‹æ•°é‡
                    all_cases_used = min(len(all_cases), 3 + stagnation_count)
                    optimization_context['all_cases'] = all_cases[:all_cases_used]
                    logger.info(f"å¢åŠ æµ‹è¯•æ¡ˆä¾‹æ•°é‡è‡³: {all_cases_used}")
            else:
                stagnation_count = 0
            
            if progress['status'] == 'improving':
                logger.info("ğŸ“ˆ ä¼˜åŒ–æ•ˆæœè‰¯å¥½ï¼Œç»§ç»­å½“å‰ç­–ç•¥")
            
            logger.info("\nã€æœ¬è½®promptã€‘")
            logger.info("-" * 40)
            logger.info(new_prompt)
            logger.info("-" * 40 + "\n")
            
        except Exception as e:
            logger.error(f"âœ— ç”Ÿæˆpromptå¤±è´¥: {str(e)}")
            new_prompt = best_prompt
        
        logger.info("2ï¸âƒ£ å¼€å§‹æµ‹è¯•æ–°prompt...")
        total_overlap = 0.0
        total_files = 0
        all_results = []
        
        for case in all_cases:
            case_id = case['case_id']
            input_data = case['input_data']
            gt_data = case['gt_data']
            
            try:
                predicted_diseases = optimizer.predict_diseases(new_prompt, input_data)
                processed_predictions = process_input_data(predicted_diseases)
                processed_gt = process_input_data(gt_data)
                
                overlap_result = calculate_overlap(processed_predictions, processed_gt)
                current_overlap = overlap_result['score']
                
                logger.info(f"\næ¡ˆä¾‹ {case_id}:")
                logger.info(f"é¢„æµ‹ç–¾ç—…ï¼š{', '.join(processed_predictions)}")
                logger.info(f"å®é™…ç–¾ç—…ï¼š{', '.join(processed_gt)}")
                logger.info(f"é‡åˆç¨‹åº¦ï¼š{current_overlap:.2%}")
                if overlap_result['missed_diseases']:
                    logger.info(f"æ¼è¯Šç–¾ç—…ï¼š{', '.join(overlap_result['missed_diseases'])}")
                if overlap_result['extra_diseases']:
                    logger.info(f"è¯¯è¯Šç–¾ç—…ï¼š{', '.join(overlap_result['extra_diseases'])}")
                
                total_overlap += current_overlap
                total_files += 1
                all_results.append({
                    'case_id': case_id,
                    'iteration': i + 1,
                    'predicted': processed_predictions,
                    'ground_truth': processed_gt,
                    'overlap': current_overlap
                })
                
            except Exception as e:
                logger.error(f"æ¡ˆä¾‹ {case_id} å¤„ç†å¤±è´¥: {str(e)}")
                continue
        
        avg_overlap = total_overlap / total_files if total_files > 0 else 0
        logger.info("\n" + "-" * 40)
        logger.info(f"æœ¬è½®å¹³å‡é‡åˆåº¦ï¼š{avg_overlap:.2%}")
        
        if avg_overlap > best_avg_overlap:
            improvement = avg_overlap - best_avg_overlap
            best_avg_overlap = avg_overlap
            best_prompt = new_prompt
            logger.info(f"\nâœ¨ å‘ç°æ›´ä¼˜prompt! æå‡äº† {improvement:.2%}")
            logger.info(f"æœ€ä½³é‡åˆåº¦ï¼š{best_avg_overlap:.2%}")
            
            # ä¿å­˜æ¯ä¸ªé˜¶æ®µçš„æœ€ä½³prompt
            with open(f'results/best_prompt_{i+1}.txt', 'w', encoding='utf-8') as f:
                f.write(best_prompt)
            
            if avg_overlap == 1.0:
                logger.info("\nğŸ‰ è¾¾åˆ°å®Œå…¨é‡åˆï¼Œæå‰ç»“æŸä¼˜åŒ–!")
                break
        
        # ä¿å­˜æœ¬è½®ç»“æœ
        results_file = os.path.join('logs', f'iteration_{i+1}_results.json')
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump({
                'iteration': i + 1,
                'avg_overlap': avg_overlap,
                'best_avg_overlap': best_avg_overlap,
                'results': all_results
            }, f, ensure_ascii=False, indent=2)
        
        logger.info("\n3ï¸âƒ£ DeepSeekåˆ†æå™¨æ­£åœ¨åˆ†ææœ¬è½®ç»“æœ...")
        # è¿™é‡Œå¯ä»¥æ·»åŠ åˆ†æå™¨çš„å…·ä½“å®ç°
        logger.info("âœ“ åˆ†æå®Œæˆ")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æå‰ç»“æŸ
        if stagnation_count >= 5:  # è¿ç»­5è½®æ— æ˜¾è‘—æ”¹è¿›
            logger.info("\nğŸ›‘ æ£€æµ‹åˆ°ä¼˜åŒ–å·²ç»åœæ»ï¼Œæå‰ç»“æŸä¼˜åŒ–")
            break
    
    # ä¿å­˜æœ€ç»ˆçš„æœ€ä½³prompt
    with open('results/best_prompt.txt', 'w', encoding='utf-8') as f:
        f.write(best_prompt)
    
    # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
    optimization_report = {
        'total_iterations': i + 1,
        'best_accuracy': best_avg_overlap,
        'optimization_status': optimizer.evaluate_optimization_progress(),
        'timestamp': timestamp
    }
    
    with open(os.path.join('results', 'optimization_report.json'), 'w', encoding='utf-8') as f:
        json.dump(optimization_report, f, ensure_ascii=False, indent=2)
    
    logger.info("="*50)
    logger.info("ä¼˜åŒ–å®Œæˆ!")
    logger.info(f"æ€»è¿­ä»£æ¬¡æ•°ï¼š{i + 1}")
    logger.info(f"æœ€ç»ˆæœ€ä½³é‡åˆåº¦ï¼š{best_avg_overlap:.2%}")
    logger.info(f"æœ€ä½³promptå·²ä¿å­˜è‡³ï¼šresults/best_prompt.txt")
    logger.info(f"å®Œæ•´ä¼˜åŒ–æ—¥å¿—å·²ä¿å­˜è‡³ï¼š{log_file}")
    logger.info(f"ä¼˜åŒ–æŠ¥å‘Šå·²ä¿å­˜è‡³ï¼šresults/optimization_report.json")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logger = logging.getLogger("Main")
        logger.error(f"ç¨‹åºè¿è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
    finally:
        logger = logging.getLogger("Main")
        logger.info("\nç¨‹åºè¿è¡Œç»“æŸ")
